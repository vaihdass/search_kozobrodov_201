# README

## 1. Веб-краулер рецептов

Краулер собирает HTML-страницы рецептов с сайта [menunedeli.ru](https://menunedeli.ru) для построения поискового индекса.

### Структура проекта

```
sitemap_parser.py   # шаг 1: сбор ссылок из XML-сайтмапа
crawler.py          # шаг 2: скачивание HTML по списку ссылок
index.txt           # список скачанных страниц: ID + URL
pages/0001.html     # скачанные HTML-страницы
```

### Что скачано

179 страниц рецептов на русском языке с сайта menunedeli.ru.
Ссылки взяты из трёх XML-sitemaps (по 60 из каждого) - охватывают разные категории: супы, выпечка, салаты и т.д.
Каждый файл содержит сырой HTML без внешних ресурсов (картинки, CSS, JS не загружаются).

Посмотреть результат без запуска кода можно в папке `pages/` и файле `hw1/index.txt`.

### Deployment Manual

#### Зависимости

Python 3.10+ и библиотека `requests`:

```bash
pip install requests
```

#### Запуск

Оба скрипта запускаются из папки `hw1/`:

```bash
cd hw1
```

**Шаг 1.** Собрать список ссылок (создаёт/перезаписывает `index.txt`):

```bash
python3 sitemap_parser.py
```

**Шаг 2.** Скачать HTML-страницы по списку из `index.txt` (сохраняет в `../pages/`):

```bash
python3 crawler.py
```


## Release Notes

### Задание 1

- Сбор 180 URL рецептов из трёх XML-сайтмапов menunedeli.ru (по 60 из каждого)
- Скачивание сырого HTML через обычный GET-запрос (без рендеринга JS)
- 179 страниц успешно сохранены (1 таймаут)
- Формат файлов: `NNNN.html` с нумерацией от 0001
- Формат индекса: `NNNN URL` построчно в `index.txt`
