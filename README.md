# README

## 1. Веб-краулер рецептов

Краулер собирает HTML-страницы рецептов с сайта [menunedeli.ru](https://menunedeli.ru) для построения поискового индекса.

### Структура проекта

```
sitemap_parser.py   # шаг 1: сбор ссылок из XML-сайтмапа
crawler.py          # шаг 2: скачивание HTML по списку ссылок
index.txt           # список скачанных страниц: ID + URL
pages/0001.html     # скачанные HTML-страницы
```

### Что скачано

179 страниц рецептов на русском языке с сайта menunedeli.ru.
Ссылки взяты из трёх XML-sitemaps (по 60 из каждого) - охватывают разные категории: супы, выпечка, салаты и т.д.
Каждый файл содержит сырой HTML без внешних ресурсов (картинки, CSS, JS не загружаются).

Посмотреть результат без запуска кода можно в папке `pages/` и файле `hw1/index.txt`.

### Deployment Manual

#### Зависимости

Python 3.10+ и библиотека `requests`:

```bash
pip install requests
```

#### Запуск

Оба скрипта запускаются из папки `hw1/`:

```bash
cd hw1
```

**Шаг 1.** Собрать список ссылок (создаёт/перезаписывает `index.txt`):

```bash
python3 sitemap_parser.py
```

**Шаг 2.** Скачать HTML-страницы по списку из `index.txt` (сохраняет в `../pages/`):

```bash
python3 crawler.py
```


## 2. Токенизация и лемматизация

Из скачанных HTML-страниц извлекается текст рецептов, токенизируется и группируется по леммам.

### Структура проекта

```
hw2/tokenizer.py    # скрипт токенизации и лемматизации
```

### Пример результата

tokens.txt:
```
абрикос
абрикосам
абрикосами
```

lemmas.txt:
```
абрикос абрикос абрикосам абрикосами абрикосов абрикосы
абсолютно абсолютно
авокадо авокадо
```

### Deployment Manual

#### Зависимости

Python 3.10+ и библиотеки:

```bash
pip install beautifulsoup4 lxml pymorphy3 nltk
```

#### Запуск

```bash
cd hw2
python3 tokenizer.py
```

Скрипт прочитает HTML из `../pages/`, запишет `tokens.txt` и `lemmas.txt` в текущую папку.


## Release Notes

### Задание 1

- Сбор 180 URL рецептов из трёх XML-сайтмапов menunedeli.ru (по 60 из каждого)
- Скачивание сырого HTML через обычный GET-запрос (без рендеринга JS)
- 179 страниц успешно сохранены (1 таймаут)
- Формат файлов: `NNNN.html` с нумерацией от 0001
- Формат индекса: `NNNN URL` построчно в `index.txt`

### Задание 2

- Извлечение текста из HTML-страниц (парсинг только основного текста, рецепта в `<article class="recipe">`)
- Очистка от мусора: навигация, реклама, комментарии, скрипты
- Токенизация: regex по кириллице, фильтрация стоп-слов (использовал nltk) и служебных частей речи (использовал pymorphy3)
- Лемматизация: группировка токенов по нормальной форме
- Результат: уникальный токенов 9981 и 4551 лемма
